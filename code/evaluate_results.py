"""
Evaluate key metrics from generated predictions. Typical call is

python code/evaluate_results.py \
  --source_dataset="${EVAL_DATASET}" \
  --source_predictions="${PREDICTIONS}" \
  --monthly_investigative_capacity=${MONTHLY_INVESTIGATIVE_CAPACITY} \
  --bootstrap_iteration_count=${ITERATION_COUNT} \
  --destination_results=${EVALUATE_SUMMARY} \
  --source_txns="${EVAL_TXNS}"

Where:
    `source_dataset` - is the PARQUET file with model data for predictions (see `generate_model_dataset.py`)
    `source_predictions` - is the PARQUET data file with predictions (see `predict.py`)
    `monthly_investigative_capacity` - is the number of transactions that can be investigated per month
    `bootstrap_iteration_count` - number of iterations to use when simulating random selection of transactions
        for investigation (as a baseling)
    `destination_predictions` - JSON file into which the transactions will be saved
    `source_txns` - transactions file produced by something like `ingest_and_split.py`, which contains
        transactionAmount and transactionTime

    Saves the results that look like:
        ```
        {
            "monthly_investigative_capacity": 120,
            "average_txn_count": 2737.3846153846152,
            "average_fraud_count": 20.307692307692307,
            "month_list": [
                "2017_10",
                "2017_5",
                "2017_1",
                "2017_12",
                "2017_3",
                "2017_9",
                "2018_1",
                "2017_4",
                "2017_7",
                "2017_8",
                "2017_11",
                "2017_2",
                "2017_6"
            ],
            "full_txns_amount": 1930304.5499999998,
            "full_fraud": 26260.2,
            "fraud_loss": 4351.16,
            "baseline_fraud_loss": 25124.132866666667,
            "prevented_fraud_loss": 21909.04,
            "baseline_prevented_fraud_loss": 1136.067133333334,
            "relative_prevented_loss": 19.284987090258213
        }
        ```

        Baseline here is the amount of fraud that would be detected if 120 transactions were selected for investigations
        randomly. This is compared to what can be aghieved with the model
"""

import argparse
import json
import typing as tp
import numpy as np
import numpy.random as npr
import pandas as pd
import duckdb as ddb
import datetime as dt


#######################

def parse_arguments() ->  argparse.Namespace:
    """
    A convinience function that initialized the argument parser and extracts
    the command line arguments
    :return: parsed arguments, args = parser.parse_args()
        which will contain args.source_dataset, args.source_predictions
        args.monthly_investigative_capacity, args.bootstrap_iteration_count,
        args.destination_results, args.source_txns
    """

    # Initialize the argument parser
    parser = argparse.ArgumentParser(description="Generate predictions using a trained model")

    # Add arguments
    parser.add_argument(
        '--source_dataset',
        type=str,
        required=True,
        help='Dataset produced by `generate_model_dataset.py`, a PARQUET files with numerical fields for training'
    )
    #
    parser.add_argument(
        '--source_txns',
        type=str,
        required=True,
        help='Dataset produced by `ingest_and_split.py`, a PARQUET files with raw inputs (needed for time-stamp)'
    )
    #
    parser.add_argument(
        '--source_predictions',
        type=str,
        required=True,
        help='Predictions PARQUET file, generated by `predict.py`'
    )
    #
    parser.add_argument(
        '--monthly_investigative_capacity',
        type=int,
        required=True,
        help='Number of transactions that can be investigated per month, for this dataset'
    )
    #
    parser.add_argument(
        '--bootstrap_iteration_count',
        type=int,
        required=True,
        help='Number of iterations to use when modelling efficiency of random investigations'
    )
    #
    parser.add_argument(
        '--destination_results',
        type=str,
        required=True,
        help='Final result stored as a JSON file'
    )

    # Parse the arguments
    args = parser.parse_args()

    return args

######################

def performance_eval(
        eval_df: pd.DataFrame,
        monthly_investigative_capacity: int,
        bootstrap_iteration_count: int
)->tp.Dict[str, float]:

    """
    Calculate the key performance metrics by splitting transactions monthly and checking
    how much fraud would be prevented if one was using the model to select highest risk transactions
    compared to randomly selected transactions

    :param eval_df: dataframe that contains   score, is_fraud_flag, transactionTime, transactionAmount
    :param monthly_investigative_capacity: number of transactions that can be investigated each month
    :param bootstrap_iteration_count: number of bootstrap intrations to use to simulate random investigations
    :return: dictionary similar to:
            {'monthly_investigative_capacity': 120,
            'average_txn_count': 2737.3846153846152,
            'month_list': array(['2017_10', '2017_5', '2017_1', ],
            'full_txns_amount': 1930304.5499999998,
            'full_fraud': 26260.2,
            'fraud_loss': 4351.16, '
            baseline_fraud_loss': 25134.470300000008,
            'prevented_fraud_loss': 21909.04,
            'baseline_prevented_fraud_loss': 1125.7296999999926,
            'relative_prevented_loss': 19.4620786854963}
    """

    # make it easy to group by year-months
    txn_dt_arr = eval_df.transactionTime
    year_month_arr = [
        f'{year}_{month}' for year, month in zip(txn_dt_arr.dt.year, txn_dt_arr.dt.month)
    ]
    #
    eval_df = eval_df.assign(year_month=year_month_arr)

    monthly_dict_list = []
    #
    for cur_year_month, sub_df in eval_df.groupby('year_month'):
        # sort by score, highest first
        sub_df = sub_df.sort_values('score', ascending=False)

        # ammount of fraud in every transaction
        cur_full_fraud_arr = (sub_df.is_fraud_flag * sub_df.transactionAmount).values

        cur_txns = sub_df.transactionAmount.sum()
        cur_full_fraud = np.sum(cur_full_fraud_arr)

        # all the transactions that were investigated did not result in
        # losses, so it is the remaining ones that lead to losss
        cur_fraud_loss = np.sum(cur_full_fraud_arr[monthly_investigative_capacity:])

        #### now simulate multiple runs where we randomly pick which transactions
        # to investigate this month
        cur_random_fraud_loss = 0.0

        for i_bootstrap_run in range(bootstrap_iteration_count):
            i_not_investigated = npr.choice(
                range(len(sub_df)),
                replace=False,
                size=len(sub_df) - monthly_investigative_capacity
            )
            ###
            cur_random_fraud_loss += np.sum(cur_full_fraud_arr[i_not_investigated])
        #
        cur_random_fraud_loss /= bootstrap_iteration_count

        cur_results_dict = {
            'year_month': cur_year_month,
            'txn_count': len(sub_df),
            'txns_sum': cur_txns,
            'fraud_count': sub_df.is_fraud_flag.sum(),
            'full_fraud_sum': cur_full_fraud,
            'fraud_loss_sum': cur_fraud_loss,
            'random_investigations_fraud_loss_sum': cur_random_fraud_loss
        }

        monthly_dict_list.append(cur_results_dict)

    monthly_df = pd.DataFrame(monthly_dict_list)

    #####
    # aggregate for the final summary
    final_summary = {
        'monthly_investigative_capacity': monthly_investigative_capacity,
        'average_fraud_count': monthly_df.fraud_count.mean(),
        'average_txn_count': monthly_df.txn_count.mean(),
        'month_list': list(eval_df.year_month.unique()),
        'full_txns_amount': monthly_df.txns_sum.sum(),
        'full_fraud': monthly_df.full_fraud_sum.sum(),
        'fraud_loss': monthly_df.fraud_loss_sum.sum(),
        'baseline_fraud_loss': monthly_df.random_investigations_fraud_loss_sum.sum()
    }
    #
    final_summary['prevented_fraud_loss'] = final_summary['full_fraud'] - final_summary['fraud_loss']
    final_summary['baseline_prevented_fraud_loss'] = final_summary['full_fraud'] - final_summary['baseline_fraud_loss']
    final_summary['relative_prevented_loss'] = final_summary['prevented_fraud_loss'] / final_summary['baseline_prevented_fraud_loss']

    return final_summary

#######################

def main(args=None)->None:
    """
    Main entrypoint, can be called by importing the file and calling function
    see `parse_arguments` for which arguments need to be passed
    :return:
    """

    if args is None:
        args = parse_arguments()

    con = ddb.connect()

    ############ load data
    # load predictions
    print(f'Loading predictions from {args.source_predictions}')
    con.execute(f"CREATE TABLE model_predictions AS SELECT eventId, score FROM '{args.source_predictions}'")

    # load labels
    print(f'Loading labels from {args.source_dataset}')
    con.execute(f"CREATE TABLE model_dataset AS SELECT eventId, is_fraud_flag FROM '{args.source_dataset}'")

    # load time-stamp
    print(f'Loading timestamp from {args.source_txns}')
    con.execute(f"CREATE TABLE txns AS SELECT eventId, transactionAmount, transactionTime FROM '{args.source_txns}'")

    eval_df = con.execute(
        f"""
        SELECT P.eventId, P.score, L.is_fraud_flag, T.transactionTime, T.transactionAmount
        FROM model_predictions AS P
        INNER JOIN model_dataset AS L
            ON L.eventId=P.eventId
        INNER JOIN txns AS T
            ON T.eventId=P.eventId
        """
    ).df()
    con.close()

    print(f'Loaded {len(eval_df)} predictions ({eval_df.is_fraud_flag.sum()} fraudulent transactions)')

    ######## now evaluate
    print('Evaluating performance')
    results_dict = performance_eval(
        eval_df=eval_df,
        monthly_investigative_capacity=args.monthly_investigative_capacity,
        bootstrap_iteration_count=args.bootstrap_iteration_count
    )

    print(results_dict)
    print(f'Saving results to {args.destination_results}')

    with open(args.destination_results, 'w') as fh:
        json.dump(results_dict, fh, indent=4)

    print('Done')

if __name__ == "__main__":
    main()
